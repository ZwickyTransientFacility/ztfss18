{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZTF alerts demo\n",
    "\n",
    "In this demo, we will:\n",
    "- Set up `Docker`\n",
    "- Using `Docker`, deploy a `MongoDB` database, fetch a night worth of (pre-filtered to reduce size) public ZTF alerts, ingest it into the database, and create indices in the database for faster queries\n",
    "- Set up `Robo3T` and use it to look at the database and query it\n",
    "- Query the database using `python`\n",
    "- Inspect the contents of an alert packet\n",
    "- Construct and plot a light curve\n",
    "- Plot the cutout images from an alert packet\n",
    "\n",
    "\n",
    "### Download and install `Docker` and `Robo3T`\n",
    "- Download and install the appropriate version of `Docker` for your platform from [here](https://www.docker.com/community-edition). You will need to create an account on their website.\n",
    "- Download and install `Robo3T` from [here](https://robomongo.org/download).\n",
    "  We will use it to connect to the database.\n",
    "\n",
    "### Fetch, build, and run the code to deploy a `MongoDB` database, fetch a night worth of (pre-filtered to reduce size) public ZTF alerts, ingest it into the database, and create indices in the database for faster queries\n",
    "\n",
    "This is a lot of stuff! Sounds scary, however with the help of `Docker`, we will only have to run a few simple commands to do all that.\n",
    "\n",
    "Clone the repo and `cd` into the directory:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dmitryduev/ztf-alerts-demo.git\n",
    "cd ztf-alerts-demo\n",
    "```\n",
    "\n",
    "The inside/contents of a `Docker` container get destroyed when it is removed, so we need to tell `Docker` to keep the useful data in a \"persistent\" storage.\n",
    "\n",
    "Create a so-called persistent `Docker` volume for `MongoDB`:\n",
    "```bash\n",
    "docker volume create alert-fetcher-mongo-volume\n",
    "```\n",
    "\n",
    "Launch the `MongoDB` container. (Feel free to change u/p for the db admin)\n",
    "```bash\n",
    "docker run -d --restart always --name alert-fetcher-mongo -p 27018:27017 \\\n",
    "       -v alert-fetcher-mongo-volume:/data/db \\\n",
    "       -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\\n",
    "       -e MONGO_INITDB_ROOT_PASSWORD=mongoadminsecret \\\n",
    "       mongo:latest\n",
    "```\n",
    "\n",
    "That's it for the database set-up! You can now connect to it on `localhost` on port `27018`.\n",
    "\n",
    "Finally, build and launch the alert-fetcher container. We will bind-mount a directory on your host machine to store the alerts:\n",
    "```bash\n",
    "cd alert-fetcher\n",
    "docker build -t alert-fetcher -f Dockerfile .\n",
    "# make sure path /path/to/alerts (or whatever path you specify) exists\n",
    "docker run --rm -v /path/to/alerts:/alerts \\\n",
    "           --name alert-fetcher --link alert-fetcher-mongo:mongo -it alert-fetcher\n",
    "```\n",
    "\n",
    "This will launch a program that will fetch a pre-filtered (demo) set of public ZTF alerts from July 13, 2018, ingest that into the MongoDB database, and create indicies to accelerate queries. The script will add a `coordinates` field to each that is not part of the original alert packets, to allow 2d indexation on the sphere for fast positional/cone searches. Additionally, a unique string `candid_objectId` is used a the alert identifier in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can now use `Robo3T` to connect to the database and take a look at the alerts. Launch `Robo3T` and click \"create\" connection. \n",
    "- Name the connection, for example, `local_docker_alerts_demo`\n",
    "- Use `localhost` : `27018` as the address\n",
    "- On the `Authentication` tab, check \"perform authentication\", and use `admin` for `Database`, `mongoadmin` for `User Name`, `mongoadminsecret` for 'Password'(if you did not choose other u/p), and `SCRAM-SHA-1` for `Auth Mechanism`\n",
    "- Click `Save` and then `Connect`\n",
    "\n",
    "`Robo3T` will connect to the database. To see the ingested alerts, click `ztf_alerts` -> `Collections` -> `alerts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Notes [optional]\n",
    "\n",
    "To check the running/stopped containers, type:\n",
    "```bash\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "To stop and remove a container, run:\n",
    "```bash\n",
    "docker stop container_name\n",
    "docker rm -f container_name\n",
    "```\n",
    "\n",
    "To get all public alerts from the ZTF archive for a given night, remove `--demo` from the last line of the file `alert-fetcher/Dockerfile` and change the date string. Then re-build and re-run the alert-fetcher container.\n",
    "\n",
    "Every time when you change the code that is used inside a container, the latter must be re-build and restarted.\n",
    "\n",
    "Alerts are stored in the database in a serialized binary format that resembles the Apache Avro format used in the packets in that it can be easily converted into `json` or a `python` dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore the contents of an avro packet. We will also learn how to construct and display a light curve for an alert and plot the cutout images.\n",
    "\n",
    "_NOTE:_ You may want to look at Eric Bellm's Jupyter [notebook](https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/notebooks/Working_with_avro_files.ipynb) with some more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `python` virtual environment with `conda` and install the required python modules into it:\n",
    "\n",
    "```bash\n",
    "conda create --name ztfalerts python=3.6\n",
    "# type y to proceed\n",
    "# install pip and jupyter:\n",
    "conda install -n ztfalerts pip\n",
    "conda install -n ztfalerts jupyter\n",
    "# activate the environment:\n",
    "source activate ztfalerts\n",
    "# download requirements.txt:\n",
    "wget https://raw.githubusercontent.com/dmitryduev/ztf-alerts-demo/master/alert-fetcher/code/requirements.txt\n",
    "# install required modules into the environment:\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fastavro\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy.io import fits\n",
    "import aplpy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Contents of an avro packet **\n",
    "\n",
    "Each avro packet contains many different pieces of information about an alert. \n",
    "You can see a description of all of the contents here:\n",
    "https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html\n",
    "\n",
    "We recommend keeping this description open in a separate tab for reference during these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at a particular avro packet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avro_packet = './alerts/20180713/558359715915010021.avro'\n",
    "with open(avro_packet, 'rb') as fa:\n",
    "    freader = fastavro.reader(fa)\n",
    "    schema = freader.schema\n",
    "\n",
    "    for packet in freader:\n",
    "        print(packet.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema is stored in the packet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload, once in memory, is a python dictionary, so the attributes are easy to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('JD: {} Filter: {} Mag: {:.2f}+/-{:.2f}'.format(\n",
    "    packet['candidate']['jd'], packet['candidate']['fid'],\n",
    "    packet['candidate']['magpsf'], packet['candidate']['sigmapsf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE ESPECIALLY:** the magnitudes here do not include the magnitude of the underlying reference source (if present), so if this is a variable star further adjustment is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Exercise: Using the schema guide linked above, answer the following questions about this candidate:\n",
    "# 1) What is the real-bogus score?\n",
    "\n",
    "print( # Finish this line\n",
    "\n",
    "# 2) What is the distance to the nearest source in Pan-STARRS? \n",
    "    \n",
    "print( # Finish this line\n",
    "        \n",
    "# 3) What is the magnitude and color of the nearest source in Pan-STARRS?\n",
    "        \n",
    "print( # Finish this line        \n",
    "\n",
    "# 4) Do you think that this candidate is a star? \n",
    "\n",
    "print( # Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record access like the above is a little verbose; let's wrap things up in a pandas dataframe for ease of access (and faster loading). As an example, we will extract the lightcurves. The alert packet formats are nested, so the historical detections (if present) have the same structure as the candidate triggering the alert (minus a couple fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataframe(packet):\n",
    "    df = pd.DataFrame(packet['candidate'], index=[0]) # the current alert\n",
    "    df_prv = pd.DataFrame(packet['prv_candidates']) # obtaining all previous alerts at this location\n",
    "    return pd.concat([df, df_prv], ignore_index=True) # we put the current alert and previous ones in the same table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dflc = make_dataframe(packet)\n",
    "dflc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the table is one alert. The first row is the most recent alert, and the remaining rows are previous alerts at the same location. Looking at the table, you can see that some of the historical detections are upper limits, signified by the NaNs. Note that the most recent candidate has a few fields that are not present for the prv_candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### The current candidate is the first line in this table, df[0]\n",
    "current_cand = df[0]\n",
    "\n",
    "### Exercise, repeat the exercises above, but this time using the df structure\n",
    "\n",
    "# 1) What is the JD, mag, filter, and uncertainty on the magnitude?\n",
    "print(df[0][ # finish this line\n",
    "\n",
    "# 2) What is the real-bogus score?\n",
    "\n",
    "print(df[0][ # Finish this line\n",
    "\n",
    "# 3) What is the distance to the nearest source in Pan-STARRS? \n",
    "    \n",
    "print(df[0][ # Finish this line\n",
    "        \n",
    "# 4) What is the magnitude and color of the nearest source in Pan-STARRS?\n",
    "        \n",
    "print(df[0][ # Finish this line\n",
    "\n",
    "# 5) Do you think that this candidate is a star? \n",
    "\n",
    "print(df[0][ # Finish this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to plotting the light curve! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_lightcurve(dflc, days_ago=True):\n",
    "    \n",
    "    filter_color = {1: 'green', 2: 'red', 3: 'pink'}\n",
    "    if days_ago:\n",
    "        now = Time.now().jd\n",
    "        t = dflc.jd - now\n",
    "        xlabel = 'Days Ago'\n",
    "    else:\n",
    "        t = dflc.jd\n",
    "        xlabel = 'Time (JD)'\n",
    "    \n",
    "    plt.figure()\n",
    "    for fid, color in filter_color.items():\n",
    "        # plot detections in this filter:\n",
    "        w = (dflc.fid == fid) & ~dflc.magpsf.isnull()\n",
    "        if np.sum(w):\n",
    "            plt.errorbar(t[w], dflc.loc[w, 'magpsf'], dflc.loc[w, 'sigmapsf'],\n",
    "                         fmt='.', color=color)\n",
    "        wnodet = (dflc.fid == fid) & dflc.magpsf.isnull()\n",
    "        if np.sum(wnodet):\n",
    "            plt.scatter(t[wnodet], dflc.loc[wnodet, 'diffmaglim'], \n",
    "                        marker='v', color=color, alpha=0.25)\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_lightcurve(dflc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out how to display the cutout images. Note that these are gzip-compressed fits files stored as bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cutout(stamp, fig=None, subplot=None, **kwargs):\n",
    "    with gzip.open(io.BytesIO(stamp), 'rb') as f:\n",
    "        with fits.open(io.BytesIO(f.read())) as hdul:\n",
    "            if fig is None:\n",
    "                fig = plt.figure(figsize=(4, 4))\n",
    "            if subplot is None:\n",
    "                subplot = (1, 1, 1)\n",
    "            ffig = aplpy.FITSFigure(hdul[0], figure=fig, subplot=subplot, **kwargs)\n",
    "            ffig.show_grayscale(stretch='arcsinh')\n",
    "    return ffig\n",
    "\n",
    "\n",
    "def show_stamps(packet):\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    for i, cutout in enumerate(['Science', 'Template', 'Difference']):\n",
    "        stamp = packet['cutout{}'.format(cutout)]['stampData']\n",
    "        ffig = plot_cutout(stamp, fig=fig, subplot=(1, 3, i+1))\n",
    "        ffig.set_title(cutout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_stamps(packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us now look at a few examples of how to query the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "\n",
    "def connect_to_db(host='localhost', port=27018, \n",
    "                  user='ztf_user', password='veryStrongPa$$word'):\n",
    "    _client = pymongo.MongoClient(host=host, port=port)\n",
    "    # grab main database:\n",
    "    _db = _client['ztf_alerts']\n",
    "    # authenticate\n",
    "    _db.authenticate(user, password)\n",
    "    \n",
    "    return _db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = connect_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all distinct transient `objectId`'s present in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objectIds = db['alerts'].distinct('objectId')\n",
    "# print ten\n",
    "print(objectIds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of alerts in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_alerts = db['alerts'].count()\n",
    "print(num_alerts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of alerts in the database with an rb score > 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_alerts_rb_gt_05 = db['alerts'].count({'candidate.rb': {'$gt': 0.5}})\n",
    "print(num_alerts_rb_gt_05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all transient `objectId`'s detected more than once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db['alerts'].aggregate([{'$group': {'_id': '$objectId', 'count': {'$sum': 1}}}, \n",
    "                                 {'$match': {'count': {'$gt': 1}}}, \n",
    "                                 {'$project': {'objectId': '$_id', '_id': 0}}], \n",
    "                                allowDiskUse=True)\n",
    "for alert in cursor:\n",
    "    print(alert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all alert `objectId`'s for transients with more than one detection in R and i bands, each with an rb score of >= 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db['alerts'].aggregate([{'$group': {'_id': '$objectId',\n",
    "                                             'count': {'$sum': {'$cond': [{'$and': [\n",
    "                                                 {'$in': ['$candidate.fid', [2, 3]]},\n",
    "                                                 {'$gt': ['$candidate.rb', 0.3]}\n",
    "                                             ]\n",
    "                                             }, 1, 0]\n",
    "                                             }\n",
    "                                             }}},\n",
    "                                 {'$match': {'count': {'$gt': 1}}},\n",
    "                                 {'$project': {'objectId': '$_id', '_id': 0}}], allowDiskUse=True)\n",
    "for alert in cursor:\n",
    "    print(alert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get alerts with `objectId` `ZTF18abgladq`. Plot the light curve of the second one and display the cutout images for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db['alerts'].find({'objectId': {'$eq': 'ZTF18abgladq'}})\n",
    "\n",
    "alerts = [alert for alert in cursor]\n",
    "\n",
    "plot_lightcurve(make_dataframe(alerts[1]))\n",
    "show_stamps(alerts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The `coordinates.radec_geojson` field defined for every object in the database has an associated spherical 2D index, which allows for extremely fast positional queries. `MongoDB` supports many query operators, see [here](https://docs.mongodb.com/manual/reference/operator/query-geospatial/) for more details. The caveat to keep in mind is the following: `MongoDB` uses `GeoJSON` objects to represent `2D` positions on the sphere. Both the longitude (RA) and latitude (Dec) must be expressed in decimal degrees, and the valid longitude values are between `-180` and `180`, both inclusive, so you must subtract 180.0 degrees from your RA value.\n",
    "\n",
    "Let's define a helper function for cone searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cone_search(_db, collection='alerts',\n",
    "                ra=0.0, dec=0.0, radius=1.0, unit='arcsec',\n",
    "                projection=None):\n",
    "    # cone search radius:\n",
    "    cone_search_radius = float(radius)\n",
    "    # convert to rad:\n",
    "    if unit == 'arcsec':\n",
    "        cone_search_radius *= np.pi / 180.0 / 3600.\n",
    "    elif unit == 'arcmin':\n",
    "        cone_search_radius *= np.pi / 180.0 / 60.\n",
    "    elif unit == 'deg':\n",
    "        cone_search_radius *= np.pi / 180.0\n",
    "    elif unit == 'rad':\n",
    "        cone_search_radius *= 1\n",
    "    else:\n",
    "        raise Exception('Unknown cone search unit. Must be in [deg, rad, arcsec, arcmin]')\n",
    "    \n",
    "    # fields to return: everything by default\n",
    "    if projection is None:\n",
    "        projection = dict()\n",
    "    \n",
    "    cursor = _db[collection].find({'coordinates.radec_geojson': {\n",
    "        '$geoWithin': {'$centerSphere':\n",
    "                           [[ra - 180.0, dec],\n",
    "                            cone_search_radius]\n",
    "                       }\n",
    "    }}, projection)\n",
    "    \n",
    "    return cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look for alerts within 30 arcsec from RA, Dec: (239.71 deg, 14.97 deg) and get their `objectId`'s and `rb` scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alerts = list(cone_search(db, ra=239.71, dec=14.97, radius=30.0, unit='arcsec',\n",
    "                          projection={'_id': 0, 'objectId': 1, 'candidate.rb': 1}))\n",
    "print(alerts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
