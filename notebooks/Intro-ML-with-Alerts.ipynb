{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Alerts\n",
    "\n",
    "The objective of this hands-on activity is to create and evaluate a Real-Bogus classifier using ZTF alert data.  You will be provided with a curated set of data which has already been labeled via the marshalls and the zooniverse website.  In this exercise, you will:\n",
    "\n",
    "1. Load data\n",
    "2. Examine and select features\n",
    "3. Curate a test and training set\n",
    "4. Train a machine learned classifier\n",
    "5. Compare the performance of different learning algorithms\n",
    "6. [ optional ] experiment with alternate feature selections and compare performance\n",
    "7. [ optional ] label additional examples and incorporate into training set\n",
    "\n",
    "#### What's Not Covered\n",
    "\n",
    "There are many topics to cover, and due to time constraints, we cannot cover them all.  Omitted is a discussion of [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html) and [hyperparameter tuning](http://scikit-learn.org/stable/modules/grid_search.html).  I encourage you to click through and read those articles by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Visit the [ZTFSS github repo](https://github.com/ZwickyTransientFacility/ztfss18/tree/master/data) to retrieve the file labeled *labeled.npy*.  This file contains data that has been labeled either 0 (BOGUS) or 1 (REAL).  The final column in the npy file contains label.\n",
    "\n",
    "The columns of the *labeled.npy* are:\n",
    "\n",
    "[ magpsf, sigmapsf, chipsf, magap, sigmagap, magapbig, sigmagapbig, sky, magdiff, fwhm, classtar, mindtoedge, magfromlim, seeratio, aimage, bimage, aimagerat, bimagerat, elong, nneg, nbad, sumrat, distnr, magnr, sigmagnr, chinr, sharpnr, ranr, decnr, ssdistnr, ssmagnr, scorr, scimaglim, refmaglim, zpmaginpsci, zpmaginpsciunc, zpdiff, zpref, fluxrat, scigain, scisat, scibckgnd, scisigpix, sciinpseeing, refsat, refbckgnd, refsigpix, refinpseeing, pdiffbckgnd, ndiffbckgnd, diffsigpix, diffnbadpixbef, diffnbadpixaft, diffpctbad, diffmaglim, difffwhm, difnumnoisepix, diffavgsqbef, diffavgsqaft, diffavgsqchg, diffavgchisqaft, infobitssci, infobitsref, ncandscimrefraw, ncandscimreffilt, ncandrefmsciraw, ncandrefmscifilt, status] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dat = np.load('../data/ml-alerts/labeled.npy')\n",
    "\n",
    "COL_NAMES = [\"magpsf\", \"sigmapsf\", \"chipsf\", \"magap\", \"sigmagap\", \"magapbig\", \"sigmagapbig\", \"sky\", \n",
    "            \"magdiff\", \"fwhm\", \"classtar\", \"mindtoedge\", \"magfromlim\", \"seeratio\", \"aimage\", \"bimage\", \n",
    "            \"aimagerat\", \"bimagerat\", \"elong\", \"nneg\", \"nbad\", \"sumrat\", \"distnr\", \"magnr\", \"sigmagnr\", \n",
    "            \"chinr\", \"sharpnr\", \"ranr\", \"decnr\", \"ssdistnr\", \"ssmagnr\", \"scorr\", \"scimaglim\", \"refmaglim\", \n",
    "            \"zpmaginpsci\", \"zpmaginpsciunc\", \"zpdiff\", \"zpref\", \"fluxrat\", \"scigain\", \"scisat\", \"scibckgnd\", \n",
    "            \"scisigpix\", \"sciinpseeing\", \"refsat\", \"refbckgnd\", \"refsigpix\", \"refinpseeing\", \"pdiffbckgnd\", \n",
    "            \"ndiffbckgnd\", \"diffsigpix\", \"diffnbadpixbef\", \"diffnbadpixaft\", \"diffpctbad\", \"diffmaglim\", \n",
    "            \"difffwhm\", \"difnumnoisepix\", \"diffavgsqbef\", \"diffavgsqaft\", \"diffavgsqchg\", \"diffavgchisqaft\", \n",
    "            \"infobitssci\", \"infobitsref\", \"ncandscimrefraw\", \"ncandscimreffilt\", \"ncandrefmsciraw\", \n",
    "            \"ncandrefmscifilt\", \"status\"]\n",
    "             \n",
    "# INSTRUCTION: Verify that the shape of the data is the number of COL_NAMES + 1\n",
    "#\n",
    "    \n",
    "    \n",
    "# INSTRUCTION: How many real and bogus examples are in this labeled set\n",
    "#\n",
    "real_mask = \n",
    "bogus_mask =\n",
    "print(\"Number of Real Examples: {}\".format(np.sum(real_mask)))\n",
    "print(\"Number of Bogus Examples: {}\".format(np.sum(bogus_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select Features\n",
    "\n",
    "While it may be tempting to use all features provided, some of the features may not be relevant to the concept of discriminating between real and bogus sources. \n",
    "\n",
    "The columns of the *labeled.npy* are:\n",
    "\n",
    "[ magpsf, sigmapsf, chipsf, magap, sigmagap, magapbig, sigmagapbig, sky, magdiff, fwhm, classtar, mindtoedge, magfromlim, seeratio, aimage, bimage, aimagerat, bimagerat, elong, nneg, nbad, sumrat, distnr, magnr, sigmagnr, chinr, sharpnr, ranr, decnr, ssdistnr, ssmagnr, scorr, scimaglim, refmaglim, zpmaginpsci, zpmaginpsciunc, zpdiff, zpref, fluxrat, scigain, scisat, scibckgnd, scisigpix, sciinpseeing, refsat, refbckgnd, refsigpix, refinpseeing, pdiffbckgnd, ndiffbckgnd, diffsigpix, diffnbadpixbef, diffnbadpixaft, diffpctbad, diffmaglim, difffwhm, difnumnoisepix, diffavgsqbef, diffavgsqaft, diffavgsqchg, diffavgchisqaft, infobitssci, infobitsref, ncandscimrefraw, ncandscimreffilt, ncandrefmsciraw, ncandrefmscifilt, status] \n",
    "\n",
    "Any feature related to the zero-point is not relevant (e.g.,  starts with zp*).  Similarly, classstar is a star-galaxy separation score that is considered unreliable. We've removed it.  The following are the list of features we have preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featnames = ['aimage', 'aimagerat', 'bimage', 'bimagerat', 'chinr', 'chipsf', \n",
    "             'distnr', 'elong', 'fwhm', 'magap', 'magapbig', 'magdiff', 'magfromlim', \n",
    "             'magnr', 'magpsf', 'mindtoedge', 'nbad', 'nneg', 'seeratio', 'sharpnr', \n",
    "             'sigmagap', 'sigmagapbig', 'sigmagnr', 'sigmapsf', 'sky', 'ssdistnr', \n",
    "             'ssmagnr', 'sumrat', 'diffavgchisqaft', 'diffavgsqaft', 'diffavgsqbef', \n",
    "             'diffavgsqchg', 'difffwhm', 'diffmaglim', 'diffnbadpixaft', 'diffnbadpixbef',\n",
    "             'diffpctbad', 'diffsigpix', 'difnumnoisepix', 'fluxrat', 'ncandrefmscifilt',\n",
    "             'ncandrefmsciraw', 'ncandscimreffilt', 'ncandscimrefraw', 'ndiffbckgnd',\n",
    "             'pdiffbckgnd', 'refbckgnd', 'refinpseeing', 'refmaglim', 'refsat', 'refsigpix',\n",
    "             'scibckgnd', 'scigain', 'sciinpseeing', 'scimaglim', 'scisat', 'scisigpix', 'status', 'scorr']\n",
    "\n",
    "#INSTRUCTION: filter the columns from 'dat' to just contain the features in 'feats', in the order that appears above.\n",
    "#\n",
    "\n",
    "feats_plus_label = \n",
    "\n",
    "print(\"dat.shape={}\".format(dat.shape))\n",
    "print(\"feats_plus_label.shape={}\".format(feats_plus_label.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curate a Test and Training Set\n",
    "\n",
    "We need to reserve some of our labeled data for evaluation.  This means we must split up the labeled data we have into the set used for training (training set), and the set used for evaluation (test set).  Ideally, the distribution of real and bogus examples in both the training and test sets are roughly identical.  One can use [sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and use the stratify option.  \n",
    "\n",
    "For ZTF data, we split the training and test data by date.  That way repeat observations from the same night (which might be nearly identical) cannot be split into the training and test set, and artificially inflate test performance.  Also, due to the change in survey objectives, it's possible that the test set features have drifted away from the training sets.\n",
    "\n",
    "Provided is *nid.npy* which contains the Night IDs for ZTF.  Split on nid=500 (May 16, 2018).  This should leave you with roughly 500 reals in your test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nids = np.load('../data/ml-alerts/nid.npy')\n",
    "print(nids.max())\n",
    "\n",
    "# INSTRUCTION: nid.npy contains the nids for this labeled data.\n",
    "# Split the data into separate data structures for train/test data at nid=500.\n",
    "# Verify that you have at least 500 reals in your test set.\n",
    "\n",
    "nid_mask_train = \n",
    "nid_mask_test = \n",
    "\n",
    "train_plus_label = \n",
    "test_plus_label =\n",
    "\n",
    "nreals_train = \n",
    "nbogus_train = \n",
    "nreals_test = \n",
    "nbogus_test = \n",
    "\n",
    "\n",
    "print(\"TRAIN Num Real={}, Bogus={}\".format(nreals_train, nbogus_train))    \n",
    "print(\"TEST Num Real={}, Bogus={}\".format(nreals_test, nbogus_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a Classifier\n",
    "\n",
    "#### Part 1: Separate Labels from the Features\n",
    "\n",
    "Now store the labels separately from the features.  \n",
    "\n",
    "#### Part 2: Handle Missing Values\n",
    "\n",
    "Almost ready for training.  However, sklearn will throw an error if there are NaN's in the features.  And there are some features (that end with 'nr') that have lots of NaN's.  We will need to replace the \\*nr features with a sentinel value (-999).  Any other features containing NaNs should be subject to median interpolation. Use the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) module from sklearn.  Make this imputation a subroutine, as you'll need to call it separately for the train and test data.  (Why?)\n",
    "\n",
    "Once the imputation is done, we can choose multiple classifiers to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Separate the labels from the features\n",
    "\n",
    "train_feats = \n",
    "train_labels = \n",
    "\n",
    "test_feats = \n",
    "test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Write a small routine for doing the imputation described above.\n",
    "# 1) Find any feature ending with 'nr' and substitute NaNs with -999\n",
    "# 2) For all other features, substitute NaNs with the median value of that feature\n",
    "# 3) Verify NaNs are gone\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def impute_missing(feats, featnames):\n",
    "\n",
    "    # CODE GOES HERE\n",
    "    \n",
    "    return feats\n",
    "\n",
    "train_feats = impute_missing(train_feats, featnames)\n",
    "test_feats = impute_missing(test_feats, featnames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Scaling Data\n",
    "\n",
    "With missing values handled, you're closing to training your classifiers.  However, because distance metrics can be sensitive to the scale of your data (e.g., some features span large numeric ranges, but others don't), it is important to normalize data within a standard range such as (0, 1) or with z-score normalization (scaling to unit mean and variance).  Fortunately, sklearn also makes this quite easy.  Please review sklearn's [preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module options, specifically StandardScaler which corresponds to z-score normalization and MinMaxScaler.  Please implement one.  \n",
    "\n",
    "FYI - Neural networks and Support Vector Machines (SVM) are sensitive to the scale of the data.  Decision trees (and therefore Random Forests) are not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Re-scale your data using either the MinMaxScaler or StandardScaler from sklearn\n",
    "#\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "train_feats = \n",
    "\n",
    "test_feats = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3. Train Classifiers\n",
    "\n",
    "Import a few classifiers and build models on your training data.  Some suggestions include a [Support Vector Machine](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), [Neural Net](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier), [NaiveBayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) and [K-Nearest Neighbor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).  \n",
    "\n",
    "All of these classifiers have parameters which should be tuned on an set of data that's independent of both the train and test data.  This tutorial does not cover parameter tuning, but a good review of how to tune classifiers is covered [here](http://scikit-learn.org/stable/modules/grid_search.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "knn3 = KNeighborsClassifier(3),\n",
    "svml = SVC(kernel=\"linear\", C=0.025, probability=True)\n",
    "svmr = SVC(gamma=2, C=1, probability=True)\n",
    "dtre = DecisionTreeClassifier(max_depth=5)\n",
    "rafo = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "naiv = GaussianNB()\n",
    "\n",
    "# INSTRUCTION: Train three classifiers and run on your test data. Here's an example to get your started.  \n",
    "# Which ones seems to take longer to train?\n",
    "# \n",
    "\n",
    "rafo.fit(train_feats, train_labels)\n",
    "rafo_scores = rafo.predict_proba(test_feats)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4. Plot Results\n",
    "\n",
    "In order to assess performance, we plot a histogram of the test set RB scores, comparing the distributions of the labeled reals vs. boguses.  The scores of the reals should be close to 1, while the scores of the boguses should be closer to 0.  The more separated the distribution of scores, the better performing your classifier is.\n",
    "\n",
    "Compare the score distributions of the classifiers you've trained.  Trying displaying as a cumulative distribution and as a straight histogram.  \n",
    "\n",
    "*Optional:* What would the decision thresholds be at the  5, 10 and 20% false negative rate (FNR)?  What would the decision threshold be at the 1, 10, and 20% false positive rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: create masks for the real and bogus examples of the test set\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "real_mask = \n",
    "bogus_mask = \n",
    "print('nreal={}, nbogus={}'.format(np.sum(real_mask), np.sum(bogus_mask)))\n",
    "\n",
    "# INSTRUCTION: First compare the classifiers' scores on the test reals only\n",
    "#\n",
    "\n",
    "scores_list = [rafo_scores ]\n",
    "legends = ['Random Forest'] \n",
    "colors = ['g'] \n",
    "\n",
    "\n",
    "# Comparison on Reals\n",
    "#\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "rbbins = np.arange(0,1,0.001)\n",
    "for i, scores in enumerate(scores_list):\n",
    "    # CODE GOES HERE\n",
    "\n",
    "ax.set_xlabel('RB Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xbound(0, 1)\n",
    "ax.legend(legends, loc=4)\n",
    "\n",
    "\n",
    "# Comparison on Reals\n",
    "#\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "rbbins = np.arange(0,1,0.001)\n",
    "for i, scores in enumerate(scores_list):\n",
    "    # CODE GOES HERE\n",
    "ax.set_xlabel('RB Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xbound(0, 1)\n",
    "ax.legend(legends, loc=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
